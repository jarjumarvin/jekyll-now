---
layout: post
title: An Introduction to Artificial Intelligence and Neural Networks through Fake Neurons
published: true
---


Over the past couple of months, I have been quite fascinated by **machine learning [ML]** and **artificial intelligence [AI]**. So much so that I am now even considering going into AI / ML research after I finish my bachelors degree in computer science. 
That being said there's still quite a lot of time until then so let's move on to today's project.

# What is a Neural Network?

> In neuroscience, a biological neural network is a series of interconnected neurons whose activation defines a recognizable linear pathway.The interface through which neurons interact with their neighbors usually consists of several axon terminals connected via synapses to dendrites on other neurons.

# The Structure of a Neuron

<p align="center">
  <img src="{{ "/images/neuron.png"}}" alt="Image of a Neuron"/>
</p>

The biological Neuron is a cell that receives, processes and transmits information. It can generally be broken down into four major parts:

* **Dendrites** receive signals from another neuron
* The **cell body** processes all incoming signals and fires a signal once a certain threshold value is reached
* The **axon** guides the signal towards the dendrites of other neurons
* **Synapses** connect two neurons and transmits the signal from neuron #1 to neuron #2

These connections between two neurons are **weighed**. The strength of a signal transmitted through any synapse is dependent on the [**synaptic potential**](https://en.wikipedia.org/wiki/Synaptic_potential). Such a weighed connection can either decrease the transmitted signals strenght, or increase it.

# What is an Artificial Neural Network?

> Artificial Neural Networks [ANNs] or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains.

Artificial Neural Networks **[ANN]** are systems derived from the human nervous systems. Their basic structure represents that of the nerve cells (neurons) which are present in the nervous systems of the human (animal) body.

# The Artificial Neuron

<p align="center">
  <img width="665" height="303" src="{{ "/images/artificialneuron.png"}}" alt="Diagram of an Artificial Neuron"/>
</p>

The artificial neuron is closely modeled after the biological ones present in our nervous systems:

* The **inputs** represent the signals the neuron receives from other nearby neurons
* **Weights** determine the strenght of an individual signal
* These signals are then **summed** to calculate the total input
* A **bias** is added to the total input, to allow for transformation
* An **activation function** calculates the neurons **output** from the input

## The Perceptron

The most basic form of an artificial neuron calculates it's output using an activation function called the **Heaviside step function**.

<p align="center">
  <img width="384" height="288" src="{{ "/images/512px-Dirac_distribution_CDF.svg.png" }}" alt="Heaviside step function"/>
</p>

$$				
	H[n] :=  \left\{\begin{matrix} 0, & n<0, \\ 1, & n>0 \\ \end{matrix}\right.
$$

This functions value is either zero for negative arguments and one for positive arguments. A neuron with an activation function like this is called a **perceptron**. The first implementation of such an algorithm dates back to the 1950s.

In general, a simple perceptron can be written as a mathematical function that maps it's input $\vec{x}$, a vector to it's output $y$, a binary value.

$$
	y(\vec{x}) =\varphi\left(\sum\limits_{i=0}^n\omega_ix_i + b\right)
$$

where $\vec{\omega}$ are the weights to each input, $\varphi$ is the activation function (step function here), and $b$ is the bias

$$
	\vec{\omega} = \left[\begin{matrix}\omega_0\\\omega_1\\\vdots\\\omega_n\end{matrix}\right] 
$$


In mathematical terms, the output of a neuron is equal to the output of the step function $\varphi$, given the sum of the dot product of the input / weight vectors and the bias as an input.

It's simple to think that a single perceptron is no good on it's own, given that it can only output one of two values - 0 or 1. Yet there is a class of problems that even a single perceptron can solve:

<p align="center">
  <img width="480" height="360" src="{{ "/images/figure_scatterplot.png"}}" alt="Scatter Plot"/>
</p>

Imagine two sets of points, divided by a line. Such a set is **linearly seperable**. As such, a single perceptron can learn where that line is and after enough training correctly predict which set any given point P(x, y) belongs to. That line is called the **decision boundary** and the **bias** mentioned above, allow the algorithm to shift that line along a direction.

# The Structure of Artificial Neural Networks
